6-16
前端：
1. 实现侧边栏、导航栏和主界面，添加路由、状态信息的配置，实现用户登录功能。
2. 添加了用户名显示，用户退出功能，持久化保存token和用户信息
3. 设置前置守卫，根据token判断是否登录，如果登录，则放行，否则跳转至login界面

后端：
1. 实现登录接口，设置过滤器JwtTokenAdminInterceptor，处理未登录情况，没有登录则创建一个token
2. 实现全局异常处理器GlobalExceptionHandle


6-17
前端：
1. 添加工作查询界面，设置了查询表单，根据过滤规则显示岗位信息

后端：
1. 添加PageResult返回值，为分页查询的对象
2. 添加分页查询岗位的接口，新增Job对象，用来表示岗位信息

新增爬虫功能（crawl文件夹）
1. 使用pymysql操纵数据库，database包定义了连接池对象、init数据库方法、实体父类entity和两个实体类company和job
2. utils工具包configLoader能够自动加载yml配置文件，cache能够将循环遍历索引持久化保存，爬虫中断后可恢复
3. crawl中token能够获取cookie，requestHelper封装了查询接口，query实现将输入转化为查询的params，api提供查询工作的接口，main为爬虫主程序入口


6-18
前端:
1. 岗位表单自动加载，实现分页查询
2. 新建岗位过滤组件和可排序·echart图表组件，在前端渲染图表

后端：
1. 优化查询sql，将原始job left join company修改为了where子句中的拼接，将查询时间从30s降低至0.5s
2. 新增job下的city,industry和title接口，用来查询表中这些字段的枚举
3. 新增了图表相关接口，为前端提供分析出的数据


6-19
前端:
1. 构建岗位、城市、行业、学历和经验和薪资关系分布图，并能根据过滤表单分析指定的岗位
2. 新增技能分析的抽屉界面，根据岗位名生成技能词云图
3. 新增岗位搜索界面，根据搜索过滤规则，获取岗位数据，并显示获取进度

后端:
1. 修改了图表接口，将待分析的横坐标、岗位过滤信息放在了ChartDTO中，内层查询根据规则进行过滤，外层查询对过滤表进行聚合，统计薪资平均值和个数
2. 引入kumo-core工具，编写WordCloudBuilder工具类生成词云图，并新增生成词云图的接口
3. 新加入获取所有省的省会城市、行业和岗位的接口

爬虫:
1. 新增爬虫的flask后端api，输入查询条件以及user_id，获取岗位信息，缓存信息改为存放在cache/user_id目录下
2. 新增redis数据库，存放进度条状态信息，不同进程通过redis共享变量


6-20
前端:
1. 岗位查询新增删除、批量删除功能，并能显示已查看、已投递
2. 岗位查询添加保存过滤规则会话框和加载规则会话框

后端:
1. 删除job表的lid，新增id主键和(city, companyId, jobName)唯一约束条件
2. 新增批量删除岗位的功能，同时删除user_job中的内容，job新增sent_cv和viewed属性
3. jobPageDTO新增user_id属性，job/page根据user_id只返回该用户查询的岗位信息
4. 新增批量添加岗位的接口，先判断企业是否存在，不存在就插入company，再判断job是否重复，没有则插入job，并得到jobId，再判断(userId,jobId)是否重复，没有则插入user_job
5. 新增user_rule表，新增规则的增删查接口

爬虫:
1. 将原始本地提交岗位信息修改为post /job/insert
2. 不用redis了，使用manager实现进程间的通信


6-21
前端:
1. 添加用户注册、个人中心功能，并对密码进行md5加密
2. 将岗位表抽象为组件，并且在岗位搜索界面中显示当前用户根据过滤规则获取到的历史数据
3. 添加用户注销界面，对所有输入框设置Enter自动跳转下一条输入框功能

后端:
1. 添加用户注册、修改密码、注销用户的接口
2. 添加了根据过滤规则md5值、用户id选择爬取数据的接口，可以获取历史过滤规则爬取到的数据

爬虫:
1. 将用户爬虫Index缓存文件名添加了由前端传递的filter的md5值，不同过滤规则可以重新加载

6-29
后端：
1. 添加根据jobIds查询岗位的接口

爬虫：
1. 修改了获取岗位列表的url和请求cookie，修改TokenFetcher的get，post参数，支持多进程用户请求
2. 添加简历上传、解析的功能，转为txt、单张图片和摘要
3. 能够自动化投递简历，发送问候语和简历图片，实现批量投递简历


6-30
前端：
1. 添加简历上传界面

后端：
1. 添加简历上传接口
2. 新增问候语润色功能，能够根据用户简历摘要，根据每一条岗位信息对问候语进行针对性润色


7-1
前端:
1. 添加简历显示、删除功能
2. 完成简历上传窗口，显示简历列表，并选择简历渲染解析结果

后端：
1. 提供上传、删除、查看简历列表的接口
2. 提供简历解析接口，并保存解析结果和时间，可以重新加载



7-2
前端:
1. 完成简历投递中的获取岗位界面
2. 新增删除全部过滤规则获取到岗位的功能
3. 岗位和最大条数的v-model绑定到一起去了，filterHash中不应该有limitNum的，添加历史记录，显示过往搜索的结果

后端:
1. 新增是否需要补全岗位描述的接口，如果岗位存在且描述不为空，或者已经投递了简历，则不新增
2. 新增删除过滤规则获取的全部岗位，但是只删除了user_job的内容，job表的不会被删除，并修改了判断是否需要更新岗位描述的条件，由where连接变为了左连接

爬虫:
1. 新增爬虫接口，获取岗位信息+岗位描述，用来给大模型做岗位匹配
2. 爬取岗位描述信息的接口，进度条粒度为每一条岗位，爬取到最大limit停止
3. 将进度条状态设置为了单用户的（一个进程一个用户）


7-3
前端:
1. 完成过滤岗位界面，根据过滤规则，以及模型、温度、批处理大小等，进行过滤，并显示过滤进度

后端:
1. 新增批量投递简历、岗位过滤、岗位排序的接口
2. 添加岗位过滤进度条显示接口，并修改progress，能够获取被过滤的岗位信息
3. 添加根据userId和filterHash查询所有岗位的接口
4. 添加只删除user_job中根据userId和jobId的元素，而不删除job表中的接口


7-4
前端:
1. 对JobTable新增跨页保存多选状态的功能
2. 完善过滤界面，能够获取后端传来了待过滤的岗位，并且勾选，也可以人工进行过滤，点击删除过滤岗位按钮，删除user_job中对应的元素
3. 新增排序界面，根据简历信息，对前面过滤后的岗位进行匹配度打分，并显示分值

后端：
1. 完善过滤和排序接口，先根据filterHash、userId和jobIds获取岗位列表，再将其传到后端进行过滤排序


7-5
前端:
1. 解决crawl,filter,score界面不同的loading,status,progress共用冲突的问题
2. score界面添加根据MinScore阈值勾选不同岗位的功能
3. 新增简历投递的前端界面