6-16
前端：
1. 实现侧边栏、导航栏和主界面，添加路由、状态信息的配置，实现用户登录功能。
2. 添加了用户名显示，用户退出功能，持久化保存token和用户信息
3. 设置前置守卫，根据token判断是否登录，如果登录，则放行，否则跳转至login界面

后端：
1. 实现登录接口，设置过滤器JwtTokenAdminInterceptor，处理未登录情况，没有登录则创建一个token
2. 实现全局异常处理器GlobalExceptionHandle


6-17
前端：
1. 添加工作查询界面，设置了查询表单，根据过滤规则显示岗位信息

后端：
1. 添加PageResult返回值，为分页查询的对象
2. 添加分页查询岗位的接口，新增Job对象，用来表示岗位信息

新增爬虫功能（crawl文件夹）
1. 使用pymysql操纵数据库，database包定义了连接池对象、init数据库方法、实体父类entity和两个实体类company和job
2. utils工具包configLoader能够自动加载yml配置文件，cache能够将循环遍历索引持久化保存，爬虫中断后可恢复
3. crawl中token能够获取cookie，requestHelper封装了查询接口，query实现将输入转化为查询的params，api提供查询工作的接口，main为爬虫主程序入口


6-18
前端:
1. 岗位表单自动加载，实现分页查询
2. 新建岗位过滤组件和可排序·echart图表组件，在前端渲染图表

后端：
1. 优化查询sql，将原始job left join company修改为了where子句中的拼接，将查询时间从30s降低至0.5s
2. 新增job下的city,industry和title接口，用来查询表中这些字段的枚举
3. 新增了图表相关接口，为前端提供分析出的数据


6-19
前端:
1. 构建岗位、城市、行业、学历和经验和薪资关系分布图，并能根据过滤表单分析指定的岗位
2. 新增技能分析的抽屉界面，根据岗位名生成技能词云图
3. 新增岗位搜索界面，根据搜索过滤规则，获取岗位数据，并显示获取进度

后端:
1. 修改了图表接口，将待分析的横坐标、岗位过滤信息放在了ChartDTO中，内层查询根据规则进行过滤，外层查询对过滤表进行聚合，统计薪资平均值和个数
2. 引入kumo-core工具，编写WordCloudBuilder工具类生成词云图，并新增生成词云图的接口
3. 新加入获取所有省的省会城市、行业和岗位的接口

爬虫:
1. 新增爬虫的flask后端api，输入查询条件以及user_id，获取岗位信息，缓存信息改为存放在cache/user_id目录下
2. 新增redis数据库，存放进度条状态信息，不同进程通过redis共享变量


6-20
前端:
1. 岗位查询新增删除、批量删除功能，并能显示已查看、已投递
2. 岗位查询添加保存过滤规则会话框和加载规则会话框

后端:
1. 删除job表的lid，新增id主键和(city, companyId, jobName)唯一约束条件
2. 新增批量删除岗位的功能，同时删除user_job中的内容，job新增sent_cv和viewed属性
3. jobPageDTO新增user_id属性，job/page根据user_id只返回该用户查询的岗位信息
4. 新增批量添加岗位的接口，先判断企业是否存在，不存在就插入company，再判断job是否重复，没有则插入job，并得到jobId，再判断(userId,jobId)是否重复，没有则插入user_job
5. 新增user_rule表，新增规则的增删查接口

爬虫:
1. 将原始本地提交岗位信息修改为post /job/insert
2. 不用redis了，使用manager实现进程间的通信


6-21
前端:
1. 添加用户注册、个人中心功能，并对密码进行md5加密
2. 将岗位表抽象为组件，并且在岗位搜索界面中显示当前用户根据过滤规则获取到的历史数据
3. 添加用户注销界面，对所有输入框设置Enter自动跳转下一条输入框功能

后端:
1. 添加用户注册、修改密码、注销用户的接口
2. 添加了根据过滤规则md值、用户id选择爬取数据的接口，可以获取历史过滤规则爬取到的数据

爬虫:
1. 将用户爬虫Index缓存文件名添加了由前端传递的filter的md值，不同过滤规则可以重新加载
2. TODO: 上传简历，解析文本内容，转换为图片，设置固定话术，实现批量投递简历