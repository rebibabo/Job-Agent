6-16
前端：
1. 实现侧边栏、导航栏和主界面，添加路由、状态信息的配置，实现用户登录功能。
2. 添加了用户名显示，用户退出功能，持久化保存token和用户信息
3. 设置前置守卫，根据token判断是否登录，如果登录，则放行，否则跳转至login界面

后端：
1. 实现登录接口，设置过滤器JwtTokenAdminInterceptor，处理未登录情况，没有登录则创建一个token
2. 实现全局异常处理器GlobalExceptionHandle

前后端联调实现登录功能


6-17
前端：
1. 添加工作查询界面，设置了查询表单，根据过滤规则显示岗位信息

后端：
1. 添加PageResult返回值，为分页查询的对象
2. 添加分页查询岗位的接口，新增Job对象，用来表示岗位信息

新增爬虫功能（crawl文件夹）
1. 使用pymysql操纵数据库，database包定义了连接池对象、init数据库方法、实体父类entity和两个实体类company和job
2. utils工具包configLoader能够自动加载yml配置文件，cache能够将循环遍历索引持久化保存，爬虫中断后可恢复
3. crawl中token能够获取cookie，requestHelper封装了查询接口，query实现将输入转化为查询的params，api提供查询工作的接口，main为爬虫主程序入口


6-18
前端:
1. 岗位表单自动加载，实现分页查询
2. 新建岗位过滤组件和可排序·echart图表组件，在前端渲染图表

后端：
1. 优化查询sql，将原始job left join company修改为了where子句中的拼接，将查询时间从30s降低至0.5s
2. 新增job下的city,industry和title接口，用来查询表中这些字段的枚举
3. 新增了图表相关接口，为前端提供分析出的数据

前后端联调实现岗位查询功能和图表渲染功能


6-19
前端:
1. 构建岗位、城市、行业、学历和经验和薪资关系分布图，并能根据过滤表单分析指定的岗位
2. 新增技能分析的抽屉界面，根据岗位名生成技能词云图
3. 新增岗位搜索界面，根据搜索过滤规则，获取岗位数据，并显示获取进度

后端:
1. 修改了图表接口，将待分析的横坐标、岗位过滤信息放在了ChartDTO中，内层查询根据规则进行过滤，外层查询对过滤表进行聚合，统计薪资平均值和个数
2. 引入kumo-core工具，编写WordCloudBuilder工具类生成词云图，并新增生成词云图的接口
3. 新加入获取所有省的省会城市、行业和岗位的接口

爬虫:
1. 新增爬虫的flask后端api，输入查询条件以及user_id，获取岗位信息，缓存信息改为存放在cache/user_id目录下
2. 新增redis数据库，存放进度条状态信息，不同进程通过redis共享变量


6-20
前端:
1. 岗位查询新增删除、批量删除功能，并能显示已查看、已投递
2. 岗位查询添加保存过滤规则会话框和加载规则会话框

后端:
1. 删除job表的lid，新增id主键和(city, companyId, jobName)唯一约束条件
2. 新增批量删除岗位的功能，同时删除user_job中的内容，job新增sent_cv和viewed属性
3. jobPageDTO新增user_id属性，job/page根据user_id只返回该用户查询的岗位信息
4. 新增批量添加岗位的接口，先判断企业是否存在，不存在就插入company，再判断job是否重复，没有则插入job，并得到jobId，再判断(userId,jobId)是否重复，没有则插入user_job
5. 新增user_rule表，新增规则的增删查接口

爬虫:
1. 将原始本地提交岗位信息修改为post /job/insert
2. 不用redis了，使用manager实现进程间的通信

TODO:
新增temp table，用来暂存爬取到的数据，数据包含company和job的全部字段，以及user_id，前端查询界面user_id查询的temp table中的值